## Synopsis

In this project, we are trying to pre-cluster reads, generated from original transcripts. Each transcript is a RNA molecule
constructed by concatenation of several exons. Reads are noisy copies of original transcripts which are generated by applying some insertions, deletions and shifts on transcripts.

Original transcripts and the frequency of reads generated from each transcript is unknown. The main objective of this project is to cluster reads generated from the same transcript.

## Challenges

### Length of transcripts
To compare each pair of transcripts, EDIT distance is an appropriate measurement which is equal to the minimum number of insertion and deletions
converting one transcript to the another one. By using dynamic programming, EDIT distance could be computed; however, time complexity is quadratic in terms of
strings length. One remedy for this problem is to apply hash functions on transcript to compress them to shorter signatures.
We have used Binarized neural network and minHash signature for training appropriate hash functions.

### Large scale dataset
One basic assumption of the project is that the dataset is large scale and classic cluster algorithms are too slow and are not applicable.
We have used Locality sensitive hashing for reducing candidate pairs for comparison.

## Generating dataset
generateDataset.py
pairsGenerator.py

## API Reference
Depending on the size of the project, if it is small and simple enough the reference docs can be added to the README. For medium size to larger projects it is important to at least provide a link to where the API reference docs live.

