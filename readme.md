## Synopsis

In this project, we are trying to pre-cluster reads, generated from original transcripts. Each transcript is a RNA molecule
constructed by concatenation of several exons. Reads are noisy copies of original transcripts which are generated by applying some insertions, deletions and shifts on transcripts.

Original transcripts and the frequency of reads generated from each transcript is unknown. The main objective of this project is to cluster reads generated from the same transcript.

## Challenges

### Length of transcripts
To compare each pair of transcripts, EDIT distance is an appropriate measurement which is equal to the minimum number of insertion and deletions
converting one transcript to the another one. By using dynamic programming, EDIT distance could be computed; however, time complexity is quadratic in terms of
strings length. One remedy for this problem is to apply hash functions on transcript to compress them to shorter signatures.
We have used Binarized neural network and minHash signature for training appropriate hash functions.

### Large scale dataset
One basic assumption of the project is that the dataset is large scale and classic cluster algorithms are too slow and are not applicable.
We have used Locality sensitive hashing for reducing candidate pairs for comparison.

## Generating dataset
generateDataset.py
pairsGenerator.py

## API Reference
1) Fasta file trimming: python FastaPreparation.py
2) Split Fasta file into the chunks: python SplitFile.py
3) Obtain MinHash signatures: python MPMH.py
4) Find similar candidate pairs: python MultiProcessLSH.py
5) Collect all the candidate pairs from different processes: ./collector.sh
6) Validate Candidate Pairs: python ValidateCandidatePairs.py
7) Cluster the polished graph: python Clustering.py
8) Merge small clusters: python MergeClusters.py
9) Create cluster directories as the input of CONVEX: python CreateClusterDirectories.py